from pyspark.sql import functions as F

# Get drift table
df = spark.table(
    "my_databricks.default.airbnb_pricer_inferencelogs_bigler1205_drift_metrics"
)

# Find the row with the latest window.end
latest_window_end = df.agg(
    F.max("window.end")
).collect()[0][0]

# Get latest windows metrics
latest_drift_metrics = df.filter(
    df["window.end"] == latest_window_end
)

# display(latest_drift_metrics)

# Set drift thresholds
js_threshold = 0.1
psi_threshold = 0.25

# Find rows exceeding thresholds
drift_exceeds_threshold = latest_drift_metrics.filter(
    (F.col("js_distance") > js_threshold) | 
    (F.col("population_stability_index") > psi_threshold)
)

display(drift_exceeds_threshold)

# Determine retraining
if drift_exceeds_threshold.count() > 0:
    print("Drift detected! Initiating retraining pipeline...")
else:
    print("No significant drift detected. No retraining needed.")